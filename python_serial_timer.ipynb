{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, nltk\n",
    "import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "# nltk.download('wordnet')   # for Lemmatization\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   Sentiment      1048575 non-null  int64 \n",
      " 1   SentimentText  1048575 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 16.0+ MB\n",
      "Accuracy with Naive-bayes multinomial test:  0.8311425328937957\n",
      "Accuracy with Naive-bayes multinomial train:  0.9039157931449778\n",
      "Accuracy with Naive-bayes bernoulli test:  0.7939270058142307\n",
      "Accuracy with Naive-bayes bernoulli train:  0.8518559894932166\n",
      "Accuracy with Naive-bayes multinomial tfidf:  0.7832013554882332\n",
      "Accuracy 10-fold: 0.829 (0.001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90    240099\n",
      "           4       0.73      0.45      0.56     74474\n",
      "\n",
      "    accuracy                           0.83    314573\n",
      "   macro avg       0.79      0.70      0.73    314573\n",
      "weighted avg       0.82      0.83      0.82    314573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#begin\n",
    "\n",
    "start=timer()\n",
    "# total_data = pd.read_csv(\"file:///c:/Users/fazal/Desktop/Uni/project/dataset/twitter-airline-sentiment/Tweets.csv\", encoding=\"ISO-8859-1\")\n",
    "# total_data=total_data[['tweet_id','airline_sentiment', 'text']]\n",
    "# total_data = total_data.rename(columns={'airline_sentiment': 'Sentiment','tweet_id':'ItemID','text':'SentimentText'})\n",
    "total_data = pd.read_csv(\"file:///c:/Users/fazal/Desktop/Uni/project/dataset/trainingandtestdata/training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\")\n",
    "total_data=total_data[['Sentiment', 'SentimentText']]\n",
    "# total_data=total_data.append(total_data, ignore_index=True)\n",
    "# total_data=total_data.append(total_data, ignore_index=True)\n",
    "# total_data=total_data.append(total_data, ignore_index=True)\n",
    "# total_data=total_data.append(total_data, ignore_index=True)\n",
    "with open('c:/Users/fazal/Desktop/Uni/project/dataset/contractions.json', 'r') as f:\n",
    "    contractions_dict = json.load(f)\n",
    "contractions = contractions_dict['contractions']\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# total_data.head()\n",
    "# tweet = total_data.columns.values[2]\n",
    "# sentiment = total_data.columns.values[1]\n",
    "tweet = total_data.columns.values[1]\n",
    "sentiment = total_data.columns.values[0]\n",
    "\n",
    "total_data.info()\n",
    "total_data.dropna()\n",
    "def emoji(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :') , :O\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' positiveemoji ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' positiveemoji ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-; , @-)\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' positiveemoji ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:, :-/ , :-|\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' negetiveemoji ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negetiveemoji ', tweet)\n",
    "    return tweet\n",
    "\n",
    "import re\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tweet = tweet.lower()                                             # Lowercases the string\n",
    "    tweet = re.sub('@[^\\s]+', '', tweet)                              # Removes usernames\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)   # Remove URLs\n",
    "    tweet = re.sub(r\"\\d+\", \" \", str(tweet))                           # Removes all digits\n",
    "    tweet = re.sub('&quot;',\" \", tweet)                               # Remove (&quot;) \n",
    "    tweet = emoji(tweet)                                              # Replaces Emojis\n",
    "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(tweet))                   # Removes all single characters\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in contractions:\n",
    "            tweet = tweet.replace(word, contractions[word.lower()])   # Replaces contractions\n",
    "    tweet = re.sub(r\"[^\\w\\s]\", \" \", str(tweet))                       # Removes all punctuations\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)                         # Convert more than 2 letter repetitions to 2 letter\n",
    "    tweet = re.sub(r\"\\s+\", \" \", str(tweet))                           # Replaces double spaces with single space    \n",
    "    return tweet\n",
    "\n",
    "total_data['processed_tweet'] = np.vectorize(process_tweet)(total_data[tweet])\n",
    "# total_data.head(10)\n",
    "\n",
    "#count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))    # Unigram and Bigram\n",
    "# count_vectorizer = CountVectorizer()\n",
    "final_vectorized_data = count_vectorizer.fit_transform(total_data['processed_tweet'])  \n",
    "\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, total_data[sentiment],test_size=0.3, random_state=69)\n",
    "\n",
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB   # Naive Bayes Classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# #multinomial\n",
    "model_naive = MultinomialNB().fit(X_train, y_train) \n",
    "\n",
    "predicted_naive = model_naive.predict(X_test)\n",
    "score_naive = accuracy_score(predicted_naive, y_test)\n",
    "print(\"Accuracy with Naive-bayes multinomial test: \",score_naive)\n",
    "\n",
    "predicted_naive_train = model_naive.predict(X_train)\n",
    "score_naive = accuracy_score(predicted_naive_train, y_train)\n",
    "print(\"Accuracy with Naive-bayes multinomial train: \",score_naive)\n",
    "\n",
    "\n",
    "\n",
    "#Bernouli\n",
    "model_naive_Bernoulli = BernoulliNB().fit(X_train, y_train) \n",
    "\n",
    "predicted_naive_Bernoulli = model_naive_Bernoulli.predict(X_test)\n",
    "score_naive_Bernoulli = accuracy_score(predicted_naive_Bernoulli, y_test)\n",
    "print(\"Accuracy with Naive-bayes bernoulli test: \",score_naive_Bernoulli)\n",
    "\n",
    "predicted_naive_Bernoulli = model_naive_Bernoulli.predict(X_train)\n",
    "score_naive_Bernoulli = accuracy_score(predicted_naive_Bernoulli, y_train)\n",
    "print(\"Accuracy with Naive-bayes bernoulli train: \",score_naive_Bernoulli)\n",
    "\n",
    "# multinomial tf-idf\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "predicted_naive_tfidf = clf.predict(X_test)\n",
    "score_naive = accuracy_score(predicted_naive_tfidf, y_test)\n",
    "print(\"Accuracy with Naive-bayes multinomial tfidf: \",score_naive)\n",
    "\n",
    "# predicted_naive_train_tfidf = clf.predict(X_train)\n",
    "# score_naive = accuracy_score(predicted_naive_train_tfidf, y_train)\n",
    "# print(\"Accuracy with Naive-bayes multinomial train tfidf: \",score_naive)\n",
    "\n",
    "\n",
    "# 10-fold cross validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "model = MultinomialNB()\n",
    "scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Accuracy 10-fold: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "# # 2-fold cross validation\n",
    "# from numpy import mean\n",
    "# from numpy import std\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "# model = MultinomialNB()\n",
    "# scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print('Accuracy 2-fold: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_naive))\n",
    "\n",
    "\n",
    "\n",
    "end=timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:02:10.392563\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'very fabulous' => 4\n",
      "'this is very awful' => 0\n",
      "'die' => 0\n",
      "'bye' => 0\n",
      "'good perfect' => 4\n",
      "'you are a fabulous person' => 4\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['very fabulous', 'this is very awful', 'die','bye','good perfect','you are a fabulous person']\n",
    "X_new_counts = count_vectorizer.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc,category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
